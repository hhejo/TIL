# 220128 FRI



옛날에 했던 거 복습하자



# 크롤링 핵심 코드

```python
# requests, bs4 라이브러리 가져오기
import requests
from bs4 import BeautifulSoup
# 웹 페이지 가져오기
res = requests.get('<https://www.naver.com/>')
# 웹 페이지 파싱하기
soup = BeautifulSoup(res.content, 'html.parser')
# 필요한 데이터 추출하기
data = soup.select_one('title')
# 추출한 데이터 활용하기
print(data.string)
```

------

# 코드 설명

1. 라이브러리 임포트

   `requests` 웹페이지 가져오기 라이브러리

   `bs4` (BeautifulSoup) : 웹페이지 분석(크롤링) 라이브러리

   ```python
   import requests
   from bs4 import BeautifulSoup
   ```

2. 웹페이지 가져오기

   `requests.get()`

   ```python
   url = '<https://www.naver.com/>'
   res = requests.get(url) # url 입력
   print(res.content) # HTML 파일이 res.content에 담김
   ```

3. 웹페이지 파싱하기

   `.content()`, `'html.parser'`

   ```python
   soup = BeautifulSoup(res.content, 'html.parser') # html을 파싱
   ```

4. 필요한 데이터 추출하기

   `.find()`

   ```python
   data = soup.find('title')
   ```

5. 추출한 데이터 활용하기

   `.get_text()` 현재 HTML 문서의 모든 텍스트 추출 가능 (모든 하위 태그 제거)

   `.string` 태그 내 문자열 반환 (좀 더 명확한 지시 필요)

   ```python
   print(data.get_text())
   print(data.string)
   ```

------

# 태그 선택자, 데이터 처리, 응답 코드

`.find()` 태그 선택

```python
data = soup.find('p', class_='cssstyle')
data = soup.find('p', 'cssstyle')
data = soup.find('p', attrs={'id': 'center'})
data = soup.find(id='body')
```

`.find_all()` 태그 모두 선택

```python
paragraph_data = soup.find_all('p')
for paragraph in paragraph_data:
		print(paragraph.get_text())
```

`.select_one`, `.select()` 태그 선택, 태그 모두 선택 (추천)

```python
soup.select_one('div') # 태그 이름만 특정
soup.select_one('.home') # 태그 class 특정
soup.select_one('div.home') # 태그 이름과 class 모두 특정
soup.select_one('#content') # 태그 id 특정
soup.select_one('div#content') # 태그 이름과 id 모두 특정
soup.select_one('div.home#content') # 태그 이름과 class, id 모두 특정
```

붙여 쓰면 해당 태그를 더 자세히 나타낼 수 있음

띄어 쓰면 하위 태그로 검색

비교

find(), select()로 가져온 객체에는 다시 find(), select() 함수 적용 가능 (서로 호환 가능)

select_one()을 쓰자

```python
soup.find('div').find('p') # find()
soup.select_one('div > p') # select_one()
```

데이터 처리

```python
.strip()`, `.split()`, `문자열 슬라이싱
```

응답 코드

HTTP response code

requests 라이브러리의 경우, requests.get()의 리턴 변수 `.status_code`에서 응답 코드 확인

200이 아니면 문제가 발생했다는 뜻

```python
import requests
res = requests.get('<https://www.naver.com/>')
if res.status_code != 200:
		print('Error')
else:
		print('OK')
```

------

# 기타

또 다른 라이브러리

```python
urllib
from urllib.requests import urlopen # urllib.requests 사용
from bs4 import BeautifulSoup
res = urlopen('<https://www.naver.com/>') # urlopen() 사용
soup = BeautifulSoup(res, 'html.parser') # 객체 들어감
print(soup.select_one('body'))
```

------

# CSV, 엑셀 파일 입출력

엑셀 파일로 만들기

`openpyxl` 엑셀 파일 라이브러리

`.Workbook()` 엑셀 파일 생성. 엑셀이 생성되면 디폴트 시트 생성됨

```python
import openpyxl
excel_file = openpyxl.Workbook()
```

`.active` 해당 시트 선택

`.title` 해당 시트 이름 변경

```python
excel_sheet = excel_file.active
excel_sheet.title = 'report'
```

`.append()` 열 데이터를 리스트 형태로 입력해 한 줄의 데이터 묶음을 작성

```python
excel_sheet.append(['data1', 'data2', 'data3']) # 1행 : data1 data2 data3
excel_sheet.append(['data4', 'data5', 'data6']) # 2행 : data4 data5 data6
```

`.save()` 엑셀 파일 저장

`.close()` 엑셀 파일 닫기

```python
excel_file.save('tmp.xlsx')
excel_file.close()
```

`.column_dimensions[].width` 해당 열의 너비 설정

```python
excel_sheet.column_dimensions['A'].width = 100
```

엑셀 파일 읽기

`.load_workbook()` 엑셀 파일 열기

```python
excel_file = openpyxl.load_workbook('tmp.xlsx')
```

`.sheetnames` 해당 엑셀 파일 안에 있는 시트 이름 확인. 리스트 타입으로 리턴

```python
excel_file.sheetnames
```

해당 엑셀 파일 안에 있는 특정 시트 선택

```python
excel_sheet = excel_file['상품정보']
```

`.rows()` 시트 안에 있는 행 읽기

`.rows()[]` 해당 행의 열 선택

`.value` 해당 셀의 내용

```python
for item in excel_sheet.rows():
		print(item[0].value, item[1].value)
```

CSV 파일로 만들기

```python
csv`, `open()`, `.writer()`, `.writerow()
import csv
def save_to_file(jobs):
    file = open('jobs.csv', mode='w', encoding='utf-8')
    writer = csv.writer(file)
    writer.writerow(['title', 'company', 'location', 'link'])
    for job in jobs:
        writer.writerow(list(job.values()))
    return
```

------

# API, JSON, XML

API

Application Programming Interface

특정 프로그램을 만들기 위해 제공되는 모듈(함수 등)을 의미

Open API

공개 API. 누구나 사용할 수 있도록 공개된 API (주로 Rest API 기술이 사용됨)

Rest API

Representational State Transfer API

HTTP 프로토콜을 통해 서버 제공 기능을 사용할 수 있는 함수

일반적으로 XML, JSON의 형태로 응답을 전달

JSON

JavaScript Object Notation

웹 환경에서 서버와 클라이언트 사이에 데이터를 주고받을 때 많이 사용 (Rest API)

```python
json`, `.loads()
import json
data = """""" # json 데이터가 있다고 생각
json_data = json.loads(data) # 이후 딕셔너리처럼 사용
```

웹 서비스 URL 설정

```
<http://localhost>
127.0.0.1
```

애플리케이션 정보

Client ID, Client Secret

네이버 Open API 예제

```Python
pprint`, `.pprint()`, `.text
import requests
import pprint
client_id = 'YOUR_CLIENT_ID'
client_secret = 'YOUR_CLIENT_SECRET'
naver_open_api = 'NAVER_OPEN_API_ADDRESS'
header_params = {'X-Naver-Client-Id': client_id, 'X-Naver-Client-Secret': client_secret}
res = requests.get(naver_open_api, headers=header_params) # header 파라미터 사용
print(res.content) # 가공되지 않은 내용
print(res.json()) # json 형식 데이터
print(res.text) # txt 형식
pprint.pprint(res.json()) # 보기 좋게 출력
```

XML

Extensible Markup Language

특정 목적에 따라 데이터를 태그로 감싸서 마크업하는 범용적인 포맷

HTML과 마찬가지로 데이터를 계층 구조로 표현

```markup
<tag attribute="att_value">content</tag>
```

------

# 정규 표현식

문자열 관련 함수

```
.count()
.index()
.find()`, `.rfind()
.join()
.strip()`, `.lstrip()`, `.rstrip()
.upper()`, `.lower()
.split()
.replace()
```

정규 표현식

regular expression : 특정한 규칙을 가진 문자열의 집합을 표현하는 데 사용하는 방식

`re` 정규 표현식 라이브러리

```python
import re
s = '<https://www.google.com/>'
print(re.sub('[^A-Za-z0-9]', '', s)) # httpswwwgooglecom
```

정규 표현식, 축약표현

`[0-9]`, `\\d` 숫자 찾음

`[^0-9]`, `\\D` 숫자 아닌 것을 찾음

`[\\t\\n\\r\\f\\v]`, `\\s` white space 문자를 찾음

`[^\\t\\n\\r\\f\\v]`, `\\S` white space 문자가 아닌 것을 찾음

`[A-Za-z0-9]`, `\\w` 문자, 숫자를 찾음

`[A-Za-z0-9]`, `\\W` 문자, 숫자가 아닌 것을 찾음

```
.sub()
re.sub('정규_표현식', '이것으로_대체', '대체할_문자열')
re.sub('[^A-Za-z0-9]', '', 'abc-123!') # abc123
```

`.compile()` 정규 표현식 패턴 만들기

```python
pattern = re.compile('')
```

한 개 매치

`.` \n을 제외한 모든 문자(한 개)를 의미

```python
pattern = re.compile('A.C')
pattern.search('ABC') # <re.Match object; span=(0, 3), match='ABC'>
pattern.search('AXC') # <re.Match object; span=(0, 3), match='AXC'>
pattern.search('ABBC') # None
pattern.search('AC') # None
```

dot 자체를 찾고 싶으면 `\\.` 혹은 `[.]`으로 표시

```python
pattern = re.compile('A\\.C')
pattern.search('A.C') # <re.Match object; span=(0, 3), match='A.C'>
pattern.search('AC') # None
```

찾고 바꾸기

```python
s = "ABC AXC ABBC AC"
re.sub('A.C', 'yes', s) # 'yes yes ABBC AC'
```

반복 매치

`?` 앞 문자가 0번 또는 1번 표시되는 패턴

`*` 앞 문자가 0번 또는 그 이상 반복되는 패턴

`+` 앞 문자가 1번 또는 그 이상 반복되는 패턴

```python
pattern = re.compile('A?C')
pattern.search('C') # <re.Match object; span=(0, 1), match='C'>
pattern.search('BC') # <re.Match object; span=(1, 2), match='C'>
pattern.search('AAAAAAC') # <re.Match object; span=(5, 7), match='AC'>
pattern = re.compile('A*C')
pattern.search('C') # <re.Match object; span=(0, 1), match='C'>
pattern.search('AC') # <re.Match object; span=(0, 2), match='AC'>
pattern.search('AAAAAAAAC') # <re.Match object; span=(0, 9), match='AAAAAAAAC'>
pattern = re.compile('A*C')
pattern.search('C') # None
pattern.search('AC') # <re.Match object; span=(0, 2), match='AC'>
pattern.search('AAAAAAC') # <re.Match object; span=(0, 7), match='AAAAAAC'>
```

`{n}` 앞 문자가 n번 반복되는 패턴

`{m,n}` 앞 문자가 m번 반복되는 패턴부터 n번 반복되는 패턴까지

```python
pattern = re.compile('A{2}C')
pattern.search('AC') # None
pattern.search('AAC') # <re.Match object; span=(0, 3), match='AAC'>
pattern.search('AAAC') # <re.Match object; span=(1, 4), match='AAC'>
pattern = re.compile('A{2,4}C')
pattern.search('AAAC') # <re.Match object; span=(0, 4), match='AAAC'>
pattern.search('AAAAC') # <re.Match object; span=(0, 5), match='AAAAC'>
pattern.search('AAAAAC') # <re.Match object; span=(1, 6), match='AAAAC'>
```

문자 매치

`[]` 괄호 안에 들어가는 문자가 들어 있는 패턴

```python
pattern = re.compile('[abcdefgABCDEF]')
pattern.search('ab1234') # <re.Match object; span=(0, 1), match='a'>
pattern.search('xy5678') # None
```

축약

`-` 알파벳을 줄여 나타냄

```python
pattern = re.compile('[a-fgA-F]')
pattern.search('ab1234') # <re.Match object; span=(0, 1), match='a'>
```

반대

`^` 괄호 `[]` 안에서 바로 뒤에 쓰면 그 뒤에 오는 문자가 아닌 패턴을 찾음

한글

```python
pattern = re.compile('[가-힣]')
pattern.search('파이썬') # <re.Match object; span=(0, 1), match='파'>
```

정리

```python
pattern = re.compile('[a-zA-Z]+')
matched = pattern.search('Python')
print(matched) # <re.Match object; span=(0, 6), match='Python'>
```

`.match()` 문자열 처음부터 정규식과 매칭되는 패턴을 찾아서 리턴

`.search()` 문자열 전체를 검색해서 정규식과 매칭되는 패턴을 찾아서 리턴

```python
pattern = re.compile('[a-z]+')
searched = pattern.search('Python') # <re.Match object; span=(1, 6), match='ython'>
matched = pattern.match('Python') # None. 맨 처음 문자 매치 안되고 종료됨
```

`.findall()` 정규표현식과 매칭되는 모든 문자열을 리스트 객체로 리턴

```python
pattern = re.compile('[a-z]+')
findalled = pattern.findall('Hello World Fun Python')
print(findalled) # ['ello', 'orld', 'un', 'ython']
pattern = re.compile('[A-Za-z]+')
findalled = pattern.findall('Hello World Fun Python')
print(findalled) # ['Hello', 'World', 'Fun', 'Python']
```

정규 표현식에 해당되는 문자열이 있는지 없는지 확인하기

```python
pattern = re.compile('[a-z]+')
findalled = pattern.findall('PYTHON')
if len(findalled) > 0:
    print ("yes")
else:
    print ("no") # no
```

`.split()` 찾은 정규 표현식 패턴 문자열을 기준으로 문자열을 분리

```python
pattern = re.compile(':')
splited = pattern.split('python:flask')
print(splited)
```























```python
pip install selenium
pip install beautifulsoup4
pip install lxml
from selenium import webdriver
browser = webdriver.Chrome("C:/dev_python/Webdriver")
browser.maximize_window()
browser.get("<https://naver.com>")
browser.execute_script("window.scrollTo(0, 1080)")  # 1920 x 1080
browser.refresh()
browser.back()
browser.forward()
elem = browser.find_element_by_css_selector("#query")
from selenium.webdriver.common.keys import Keys
elem.send_keys("나도코딩")
elem.send_keys(Keys.ENTER)
elem.click()
elem.get_attribute("href")
print(browser.page_source)
browser.find_element_by_link_text("가는날 선택").click()
browser.find_elements_by_link_text("27")[0].click()
browser.find_elements_by_link_text("28")[0].click()

from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
try:
	elem = WebDriverWait(browser, 10).until(EC.presence_of_element_located((By.XPATH, "xpath!")))
finally:
	browser.quit()

browser.close()
browser.quit()
browser.execute_script("window.scrollTo(0, document.body.scrollHeight)")
import time
interval = 2
prev_height = browser.execute_script("return document.body.scrollHeight")
while True:
	# 스크롤을 가장 아래로 내림
	browser.execute_script("window.scrollTo(0, document.body.scrollHeight)")
	# 페이지 로딩 대기
	time.sleep(interval)
	# 현재 문서 높이를 가져와서 저장
	curr_height = browser.execute_script("return document.body.scrollHeight")
	# 현재 문서 높이가 이전 문서 높이와 같으면 탈출
	if curr_height == prev_height:
		print("스크롤 완료")
		break
	# 이전 문서 높이를 현재 문서 높이로 갱신
	prev_height = curr_height
import requests
from bs4 import BeautifulSoup
headers = {"User-Agent": "", "Accept-Language": "ko-KR,ko"}
res = requests.get("URL", headers=headers)
res.raise_for_status()
# 위에서 셀레니움으로 스크롤 다 내린 HTML 파일
soup = BeautifulSoup(browser.page_source, "lxml")
# 리스트에 해당하는 클래스가 있으면 리턴
movies = soup.find_all("div", attrs={"class":["ImZGtf mpg5gc", "Vpfmgd"]})
print(soup.prettify())
options = webdriver.ChromeOptions()
options.headless = True
options.add_argument("window-size=1920x1080")
options.add_argument("user-agent=USER_AGENT")
browser = webdriver.Chrome(options=options)
browser.get_screenshot_as_file("google_movie.png")
```

[Selenium with Python](https://selenium-python.readthedocs.io/)

이미지 파일 저장

```python
images = soup.find_all("img", attrs={"class":"thumb_img"})
for idx, image in enumerate(images):
	image_url = image["src"]
	if image_url.startwith("//")
		image_url = "https:" + image_url
		image_res = requests.get(image_url)
		image_res.raise_for_status()
		with open(f"movie{idx + 1}.jpg", "wb") as f:
			f.write(image_res.content)
		if idx >= 4:
			break
```

csv 파일 저장

```python
import csv
filename = "시가총액1-200.csv"
# encoding : "utf8", "utf-8-sig"
f = open(filename, "w", encoding="utf-8-sig", newline="")
writer = csv.writer(f)
writer.writerow(data)
```

# 4. XPath

// : 하위 모든 것에 대해 탐색

- : 이름 상관 없이 탐색

/html/body/div[2]/div[2]/div[3]/div/div[2]/a 이 경로를 xpath로 나타내면

//*[@id="account"]/a

------

# 6. Requests

```python
import requests
res = requests.get('<https://naver.com>')
if res.status_code == requests.codes.ok:
		print('정상')
res.raise_for_status()
res = requests.get('<https://naver.com>')
res.raise_for_status() #응답코드 200이 아니면 오류를 내고 프로그램 끝냄
with open('mygoogle.html', 'w', encoding='utf8') as f:
		f.write(res.text)
```

------

# 7. 정규식 기본 1

```python
import re
p = re.compile() # p는 패턴을 의미 compile()은 컴파일 하겠다는 것 어떤 정규식을?
p = re.compile('ca.e') # .은 하나의 문자를 의미 caae, cabe, cace (O) | caffe (X)
# ^ (^de) 문자열의 시작을 의미 desk, destination, depart (O) | fade (X)
# $ (se$) 문자열의 끝을 의미 case, base (O) | face (X)

m = p.match('case') #비교하려는 값 'case'를 넣어줌 매칭된 값은 m에 들어감
print(m.group()) #case 출력됨 매칭이 됐기 때문 매칭되지 않으면 에러 발생

m = p.match('careless') #매치됨 주어진 문자열의 처음부터! 일치하는지 확인하기 때문 care
m = p.match('good care') #매치 안됨
```

------

# 8. 정규식 기본 2

```python
import re
p = re.compile()
p = re.compile('ca.e')

m = p.search('good care') #매치됨 주어진 문자열 중에 일치하는게 있는지 확인 care만 빼옴
m = p.search('careless') #매치됨 care만 빼옴
print(m.string) #입력받은 문자열 그대로 반환
print(m.group()) #일치하는 문자열 반환
print(m.start()) #일치하는 문자열의 시작 인덱스
print(m.end()) #일치하는 문자열의 끝 인덱스
print(m.span()) #일치하는 문자열의 시작과 끝 인덱스
m = p.findall('careless') #일치하는 모든 것을 리스트! 형태로 반환 care라고 나옴
m = p.findall('good care cafe') #['care', 'cafe']
```

------

# 9. User Agent

```python
import requests
res = requests.get('<http://nadocoding.tistory.com>')
#res.raise_for_status()
with open('nadocoding.html', 'w', encoding='utf8' as f:
	f.write(res.text)
#접속 실패 코드 떠서 주석으로 처리하고 파일로 저장해서 열었더니
#뭔가 비어보인다
```

user agent string이라고 구글에 검색

What is my User Agent?

Your User Agent is: .......... ← 접속하는 브라우저에 따라 다름

```python
import requests
url = '<http://nadocoding.tistory.com>'
headers = {'User-Agent':'..........'}
res = requests.get(url, headers=headers) #user-agent값을 전달
res.raise_for_status()
with open('nadocoding.html', 'w', encoding='utf8' as f:
	f.write(res.text)
#정상적으로 html문서를 받아옴
```

------

# 10. BeautifulSoup4 기본 1

lxml 패키지도 설치 (터미널에 pip install lxml)

```python
import requests
from bs4 import BeautifulSoup

url = '<https://comic.naver.com/webtoon/weekday.nhn>'
res = requests.get(url)
res.raise_for_status()

soup = Beautifulsoup(res.text, 'lxml') #lxml 파서
soup.title #<title>네이버 만화 &gt; 요일변 웹툰 &gt; 전체웹툰</title>
soup.title.get_text() #네이버 만화 > 요일별 웹툰 > 전체웹툰
soup.a #soup 객체에서 첫번째로 발견된 a 엘리먼트 정보 출력
soup.a.attrs #a 엘리먼트의 속성 정보 딕셔너리 형태로 가져옴
soup.a.attrs['href'] #href의 속성 '값' 가져옴

soup.find('a', attrs={'class': 'Nbtn_upload'}) #이거랑 아래랑
soup.find(attrs={'class': 'Nbtn_upload'}) #실행 결과가 같음 왜냐하면 soup 객체에 하나밖에 없어서
.get_text()
```

현재 태그를 포함하여 모든 하위 태그를 제거하고 유니코드 텍스트만 들어있는 문자열 반환

항상 마지막 태그에 사용해야 함

```python
.string
```

태그 내 문자열을 반환

태그 내 자식 태그가 둘 이상이면, 무엇을 반환해야 하는지 명확하지 않기 때문에 `None` 반환

자식 태그가 하나이면서, 그 자식 태그가 `.string` 값을 가지면 자식 태그의 문자열 반환

> [[파이썬\] BeautifulSoup을 이용해서 텍스트 추출하기: get_text(), string](https://hogni.tistory.com/21)

------

# 11. BeautifulSoup4 기본 2

```python
rank1 = soup.find('li', attrs={'class': 'rank01'})
rank1.a.get_text()
rank2 = rank1.next_sibling.next_sibling #형제 요소 가져옴 다음 거로
rank3 = rank2.previous_sibling.previous_sibling #전 거로 가져옴
rank1.parent #부모 요소 가져옴
rank2 = rank1.find_next_sibling('li')
rank3 = rank2.find_next_sibling('li')

webtoon = soup.find('a', text='독립일기-11화 밥공기 딜레마') #해당 텍스트 찾음
```

------

# 12. BeautifulSoup4 활용 1-1

```python
cartoons = soup.find_all('a', attrs={'class': 'title'})
for cartoon in cartoons:
	print(cartoon.get_text()) #class 속성이 title인 모든 'a' 엘리먼트 반환 텍스트로
''' ########### 외워버리자  ########### '''

import requests
from bs4 import BeautifulSoup

url = '<https://~~~~>'

result = requests.get(url)
result.raise_for_status()

soup = BeautifulSoup(result.text, 'lxml')

''' ########### 외워버리자  ########### '''
```

------

# 15. BeautifulSoup4 활용 2-2

HTTP METHOD : GET, POST

GET : 누구나 볼 수 있게 URL에 (많이 못 보냄)

POST : HTTP 메시지 바디에 숨겨서 보내는 방식 (아이디, 패스워드)

```python
items = soup.find_all('li', attrs={'class': re.compile('^search-product')})
```



























파라미터 복수로 보내기

```python
import requests

URL = 'https://api.agify.io'
name = input('Enter your name: ')
params = {'name': name}  # 파라미터를 딕셔너리로 담기
response = requests.get(URL, params=params).json()  # 매개변수에 파라미터 전달 (쿼리로 들어감)
print(response.get('age'))
```







